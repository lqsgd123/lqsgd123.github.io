---
layout:     post
title:      神经网络学习
date:       2018-10-28
author:     Sometimes Naive
header-img: img/black.png
catalog: true
tags:
    - 机器学习
---

## 背景知识

神经网络的起源是人们想创造设计出模仿人的大脑的算法。把任何传感器接入到大脑，大脑的学习算法就能找出学习数据的方法并处理这些数据。如果我们能找出大脑的学习算法并在计算机上执行，那么我们也就离真正意义上的人工智能不远了。

## 假设模型

神经元是大脑中的细胞，通过树突（输入通道）接受其他神经元的信息，通过轴突（输出通道）发送一段微弱电流（动作电位）给其他神经元传递信号或传送信息。

了解了神经元后，我们将使用一个逻辑单元（如下图）来模拟人工神经网络。蓝色圆圈代表输入，黄色圆圈代表神经元细胞体，左边带箭头的直线代表输入通道，右边则代表输出通道，假设函数$h_\theta (x)$代表输出。如果有必要，我们还会增加一个额外的节点$x_0$，称为偏置单元或偏置神经元。由于$x_0$总是等于1，是否写出取决于加上它方便与否。还有一个应该提到的便是Sigmoid（logistic）激活函数（人工神经元），激活函数指代非线性函数$g(z)$。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwmyej1onfj209506tt8p.jpg)

神经网络（如下图）实际上就是一组神经元连接在一起的集合。网络中的第一层称为**输入层**；最后一层称为**输出层**；中间层称为**隐藏层**。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwmz1en12lj20e608kdg6.jpg)

结合上图和下图，$a_i^{(i)}$称为第$j$层第$i$项的**激活项**，所谓激活项就是由一个具体神经元计算并输出的值。$\theta ^{(j)}$是**权重矩阵**，它控制从第$j$层到$J+1$层的映射。如果一个网络在第$j$层有$s_j$个单元，在第$j+1$层有$s_{j+1}$个单元，那么$\theta ^{(j)}$的维度为$s_{j+1}×(s_j+1)$。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwmzlo6x89j20cj03rt8x.jpg)

## 选择网络架构

 网络架构就是神经元之间的连接模式。选择网络架构是训练神经网络的第一步。如何选择隐藏层和隐藏单元的数量值得我们去考虑。那么，如何做出选择呢？

首先，定义输入单元的数量。一旦定义了数据集x，就定义了特征$x^{(i)}$的维度，也就是输入单元的数量。

其次，定义输出单元的数量。如果进行的是多类别分类，那么输出单元就由所要区分的类别个数确定。（注意：如果进行多类别分类，那么结果应该以向量的形式输出）

再次，选择隐藏单元和隐藏层的数目。一般的选择是只使用一个隐藏层，如果隐藏层不止一个，那么各个隐藏层通常都有相同的隐藏单元数量（通常情况下，隐藏单元越多越好。当然，计算量也会增大）。

## 随机初始化权重

关于初始化权重的第一个要回答的问题是为什么不能权重初始化为0？Ng给出的解释是，假如所有权重初始化为0，那么隐藏层的激活项将相等，误差将相等，代价函数的偏导数也将相等，那么，更新后的权重也将相等（虽然不为0），这意味着神经网络学习失去了意义。

因而，我们将采用随机初始化权重的方法。上面的问题叫做对称权重问题，随机初始化权重就是用来解决这样的问题。通常，我们希望权重初始化为非常接近0的值，但是不能为0。

## 前向传播

前向传播实际上就是计算$h_\theta (x)​$的过程。输入单元的激活项前向传播给隐藏层，然后计算隐藏层的激活项，接着继续前向传播，一直到计算输出层的激活项，这个过程就叫前向传播（如图）。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwn18ue52bj207m062aa6.jpg)

## 反向传播

我们先介绍一些神经网络的代价函数（如图），实际上这个代价函数是逻辑回归中的代价函数的一般形式，只不过$h(x)$是一个K维向量。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwnra7uma0j20ej04fjri.jpg)

在这里，我们引入一个中间变量$\delta ^{(l)}$，它代表着第$l$层的误差。反向传播的过程实际上就是从输出层计算$\delta ^{(l)}$，然后返回到前一层来计算$\delta ^{(l-1)}$，直到输入层（注意：我们并不需要计算输入层的$\delta ^{(l)}$，因为输入层不存在误差，也就不需要改变）。

输出层的误差公式为：![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwnnt3696lj202m00u0au.jpg)

隐藏层的误差公式为：![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwno00kaftj206l00y0rt.jpg)

其中，激活函数$g$的导数为：![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwno2hi2bpj206200t0ko.jpg)

那么，反向传播算法的整个过程如下：

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwnoo74llaj20ek07xwfa.jpg)

其中，$\Delta $是$\delta $的大写，将会被用来计算$J(\theta)$对$\theta $的偏导数。

关于反向传播更好的理解请参考[[一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)]，这里不再赘述。

## 梯度检验

由于反向传播在运行中可能存在一些微小的bug，因而我们需要梯度检验。

要进行梯度检验，需要我们计算数值梯度，数值梯度的计算公式如图。

在这里，我们计算的是双边数值梯度，双边数值梯度比单边更准确。

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwo05kiuasj20g807aq3g.jpg)

假如$\theta ​$是向量参数：

![](http://ww1.sinaimg.cn/large/9cc52ef9gy1fwo086ay0bj20fw08gjs1.jpg)

梯度检验的步骤：

1. 通过反向传播计算出解析梯度

2. 计算数值梯度

3. 比较数值梯度和解析梯度，确保两者相差无几


## 训练神经网络的过程

训练神经网络的过程有六步。

第一步是构建一个神经网络，然后随机初始化权重；

第二步是执行前向传播算法，计算出对于任意一个输入$x^{(i)}$对应的$h_{\theta }^{(x^{(i)})}$的值。

第三步是计算出代价函数$J(\theta )$

第四步是执行反向传播算法，计算出代价函数$J(\theta )$关于$\theta $的偏导数

第五步是使用梯度检验来对通过反向传播算法计算得到的偏导数项和用数值计算方法得到的估计值进行比较

第六步是使用最优化算法（如梯度下降法或其他更高级的）和反向传播算法计算使代价函数$J(\theta )$最小的$\theta $，在这一步中要停用梯度检验（因为其计算缓慢）。（另外，尽管代价函数$J(\theta )​$是非凸函数，得到的可能是局部最优解，但是我们依然能够得到较好的效果）

